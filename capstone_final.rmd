---
title: "CAPSTONE"
author: "Jesus"
date: "04/04/2021"
output:
  html_document:
    
    keep_md: yes
  md_document: default
  pdf_document: default
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# 1 - Introduction   
The goal of this exercise is to create a product to highlight the prediction algorithm that you have built and to provide an interface that can be accessed by others. For this project you must submit:

A Shiny app that takes as input a phrase (multiple words) in a text box input and outputs a prediction of the next word.

A slide deck consisting of no more than 5 slides created with R Studio Presenter (https://support.rstudio.com/hc/en-us/articles/200486468-Authoring-R-Presentations) pitching your algorithm and app as if you were presenting to your boss or an investor.

```{r echo =FALSE, message=FALSE, warning=FALSE}
# libraries
library(dplyr)
library(tm)
library(ggplot2)
library(stringi)
  library(stringr)
library(knitr)
library(wordcloud)
library(RColorBrewer)
Sys.setenv('JAVA_HOME'="C:/Program Files/Java/jre1.8.0_291/") 
library(RWeka)
library(tidytext)
library(tidyverse)
library(lubridate)
  library(ngram)


```

# 2 - Preparation of Data

## 2.1 -  Downloading File and Unzip

The file once is unziped , has an english folder that contais three files: **en_US.blogs.txt**, **en_US.news.txt**, and **en_US.twitter.txt**.

```{r}



if(!file.exists("Coursera-SwiftKey.zip")) {
      download.file("https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip", "Coursera-SwiftKey.zip")
      unzip("Coursera-SwiftKey.zip")
}

```

## 2.2 -  Extracting the lines

We extract the lines of each of the three files:

```{r}

# The name of the files Files:

blogs_file   <- "./final/en_US/en_US.blogs.txt"
news_file    <- "./final/en_US/en_US.news.txt"
twitter_file <- "./final/en_US/en_US.twitter.txt" 

#Lines extraction
blogs_lines   <- readLines(blogs_file, skipNul = TRUE)
news_lines    <- readLines(news_file,  skipNul = TRUE)
twitter_lines <- readLines(twitter_file, skipNul = TRUE)

#Conversion into dataframe
blogs_lines   <- data_frame(text = blogs_lines)
news_lines    <- data_frame(text = news_lines)
twitter_lines <- data_frame(text = twitter_lines)

```

## 2.3 -  Sample an fusion of the data

In order to manipulate the data, we must sample it for each file using the sample_n function. Finally we  make one only file with the union of the three files :

```{r}



set.seed(1001)
pct <- 0.05 #sample reduction %

blogs_sample_lines <- blogs_lines %>% sample_n(., nrow(blogs_lines)*pct)
news_sample_lines <- news_lines %>% sample_n(., nrow(news_lines)*pct)
twitter_sample_lines <- twitter_lines %>% sample_n(., nrow(twitter_lines)*pct)

#Sample union
sample <- bind_rows(mutate(blogs_sample_lines, source = "blogs_lines"),
                         mutate(news_sample_lines,  source = "news_lines"),
                         mutate(twitter_sample_lines, source = "twitter_lines")) 
sample$source <- as.factor(sample$source)

```
## 2.4 -  Tidy and clean the information

We clean the information, eliminating/substituting non desired characters that won't help us in our prediction final purpose:

```{r}

urls <- "http[^[:space:]]*"
spaces <- "[^[:alpha:][:space:]]*"
rest <- "\\b(?=\\w*(\\w)\\1)\\w+\\b"  


final_sample <-  sample %>%
  mutate(text = str_replace_all(text, spaces, "")) %>%
  mutate(text = str_replace_all(text, urls, "")) %>%
  mutate(text = str_replace_all(text, rest, "")) %>% 
  mutate(text = iconv(text, "ASCII//TRANSLIT")) #English


```
## 2.5 -  NGrams information

We obtain from the final cleaned sample, the n grams information that will let us the prediction analysis, using  unnest_tokens funtion. In order to obtain a faster algorythm we use only the 4th fist grams (order 2 , 3 and 4) :

```{r}

#' 2 grams  
bigrams <- final_sample  %>%
  unnest_tokens(bigram, text, token = "ngrams", n = 2)

#' 3 grams  
trigrams <- final_sample  %>%
  unnest_tokens(trigram, text, token = "ngrams", n = 3)

#' 4 grams  
quadgrams <- final_sample  %>%
  unnest_tokens(quadgram, text, token = "ngrams", n = 4)


```
## 2.6 -  NGrams reduction

The N grams information is reduced in order to make fast the prediction algorythm. Only the upper half zone of priorization is taking into account:

```{r}


bigram_red <- bigrams %>% count(bigram) %>%  mutate(proportion = n / sum(n)) %>%
  arrange(desc(proportion)) %>%  mutate(coverage = cumsum(proportion)) %>% filter(coverage <= 0.5)


trigram_red <- trigrams %>% count(trigram) %>%  mutate(proportion = n / sum(n)) %>%
  arrange(desc(proportion)) %>%  mutate(coverage = cumsum(proportion)) %>% filter(coverage <= 0.5)


quadgram_red <- quadgrams %>% count(quadgram) %>%  mutate(proportion = n / sum(n)) %>%
  arrange(desc(proportion)) %>%  mutate(coverage = cumsum(proportion)) %>% filter(coverage <= 0.5)


```
## 2.6 -  NGrams save

The information of the 2,3 4 Grams , is separated in words columns an saved in repo files (function saveRDS) in order to be used by the ShinyApp later:

```{r}
bigram_words <- bigram_red %>% separate(bigram, c("word1", "word2"), sep = " ")
trigram_words <- trigram_red %>% separate(trigram, c("word1", "word2", "word3"), sep = " ")
quadgram_words <- quadgram_red %>% separate(quadgram, c("word1", "word2", "word3", "word4"), sep = " ")

saveRDS(bigram_words, "bigram_words.rds")
saveRDS(trigram_words, "trigram_words.rds")
saveRDS(quadgram_words, "quadgram_words.rds")

```

